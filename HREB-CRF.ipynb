{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f6d568-7ab7-40e7-be54-4cf98da5c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import HREBCRF\n",
    "from utils import tokenize, compute_metrics, WeightLoggerCallback\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizerFast, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516a027a-74eb-462a-b24a-9500ac98432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'PassbyGrocer/msra-ner'\n",
    "bert_model = 'hfl/chinese-roberta-wwm-ext-large'\n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13b1d759-6c8c-4482-bece-bf4d5ec93f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HREBCRF were not initialized from the model checkpoint at hfl/chinese-roberta-wwm-ext-large and are newly initialized: ['bilstm.bias_hh_l0', 'bilstm.bias_hh_l0_reverse', 'bilstm.bias_hh_l1', 'bilstm.bias_hh_l1_reverse', 'bilstm.bias_ih_l0', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_ih_l1', 'bilstm.bias_ih_l1_reverse', 'bilstm.weight_hh_l0', 'bilstm.weight_hh_l0_reverse', 'bilstm.weight_hh_l1', 'bilstm.weight_hh_l1_reverse', 'bilstm.weight_ih_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_ih_l1', 'bilstm.weight_ih_l1_reverse', 'classifier.bias', 'classifier.weight', 'crf.end_transitions', 'crf.start_transitions', 'crf.transitions', 'layer_norm.bias', 'layer_norm.weight', 'mega.Uh', 'mega.Wh', 'mega.bh', 'mega.multi_headed_ema.alphas', 'mega.multi_headed_ema.dampen_factors', 'mega.multi_headed_ema.expansion', 'mega.multi_headed_ema.reduction', 'mega.single_headed_attn.offsetscale.beta', 'mega.single_headed_attn.offsetscale.gamma', 'mega.single_headed_attn.rel_pos_bias.relative_attention_bias.weight', 'mega.single_headed_attn.to_qk.0.bias', 'mega.single_headed_attn.to_qk.0.weight', 'mega.single_headed_attn.to_v.0.bias', 'mega.single_headed_attn.to_v.0.weight', 'mega.to_reset_gate.0.bias', 'mega.to_reset_gate.0.weight', 'mega.to_update_gate.0.bias', 'mega.to_update_gate.0.weight', 'r_lstm', 'r_mega']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/sijin/miniconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataset= load_dataset(dataset_name)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "val_dataset = dataset['validation']\n",
    "num_labels=len(train_dataset.features[\"ner_tags\"].feature.names)\n",
    "model = HREBCRF.from_pretrained(bert_model, num_labels=num_labels)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e413f6-fb87-4218-853c-374ddfc5be87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50a072d38684d9f84f076437514d9d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/46364 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1fc77e9e4b4c08bdea00bb86356e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4365 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.rename_column('ner_tags', 'label_ids')\n",
    "test_dataset = test_dataset.rename_column('ner_tags', 'label_ids')\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: tokenize(x, tokenizer, max_length), batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(lambda x: tokenize(x, tokenizer, max_length), batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0512f7-7282-48fa-b7dd-ca16c4950cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b77a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sijin/miniconda3/lib/python3.12/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at ../aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14490' max='14490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14490/14490 1:07:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>44.421100</td>\n",
       "      <td>44.428574</td>\n",
       "      <td>0.942418</td>\n",
       "      <td>0.925364</td>\n",
       "      <td>0.933683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>24.618100</td>\n",
       "      <td>40.110237</td>\n",
       "      <td>0.922253</td>\n",
       "      <td>0.914421</td>\n",
       "      <td>0.918228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>17.053600</td>\n",
       "      <td>57.471779</td>\n",
       "      <td>0.924063</td>\n",
       "      <td>0.931646</td>\n",
       "      <td>0.927705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>14.589100</td>\n",
       "      <td>76.849312</td>\n",
       "      <td>0.913823</td>\n",
       "      <td>0.928152</td>\n",
       "      <td>0.920597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.782400</td>\n",
       "      <td>89.700432</td>\n",
       "      <td>0.931988</td>\n",
       "      <td>0.924812</td>\n",
       "      <td>0.928205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>8.388100</td>\n",
       "      <td>74.115349</td>\n",
       "      <td>0.935111</td>\n",
       "      <td>0.937165</td>\n",
       "      <td>0.936002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.009000</td>\n",
       "      <td>77.435707</td>\n",
       "      <td>0.933294</td>\n",
       "      <td>0.931544</td>\n",
       "      <td>0.932287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.349900</td>\n",
       "      <td>64.528450</td>\n",
       "      <td>0.934997</td>\n",
       "      <td>0.936859</td>\n",
       "      <td>0.935725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.302200</td>\n",
       "      <td>66.708778</td>\n",
       "      <td>0.945213</td>\n",
       "      <td>0.940312</td>\n",
       "      <td>0.942670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.179500</td>\n",
       "      <td>64.482590</td>\n",
       "      <td>0.941601</td>\n",
       "      <td>0.939403</td>\n",
       "      <td>0.940371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current weight of:\n",
      "r_lstm: 0.6216\n",
      "r_mega: 0.6216\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6212\n",
      "r_mega: 0.6212\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.95      0.97      2674\n",
      "       I-LOC       0.98      0.92      0.95      4076\n",
      "       B-ORG       0.90      0.92      0.91      1218\n",
      "       I-ORG       0.94      0.93      0.93      5054\n",
      "       B-PER       0.98      0.98      0.98      1304\n",
      "       I-PER       0.98      0.98      0.98      2425\n",
      "\n",
      "   micro avg       0.96      0.94      0.95     16751\n",
      "   macro avg       0.96      0.95      0.95     16751\n",
      "weighted avg       0.96      0.94      0.95     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6209\n",
      "r_mega: 0.6209\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6209\n",
      "r_mega: 0.6209\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6207\n",
      "r_mega: 0.6207\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6205\n",
      "r_mega: 0.6205\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.95      0.97      2674\n",
      "       I-LOC       0.96      0.94      0.95      4076\n",
      "       B-ORG       0.91      0.93      0.92      1218\n",
      "       I-ORG       0.92      0.92      0.92      5054\n",
      "       B-PER       0.97      0.98      0.98      1304\n",
      "       I-PER       0.96      0.98      0.97      2425\n",
      "\n",
      "   micro avg       0.95      0.94      0.95     16751\n",
      "   macro avg       0.95      0.95      0.95     16751\n",
      "weighted avg       0.95      0.94      0.95     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6202\n",
      "r_mega: 0.6202\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6201\n",
      "r_mega: 0.6201\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6200\n",
      "r_mega: 0.6201\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6199\n",
      "r_mega: 0.6199\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.97      0.96      0.97      2674\n",
      "       I-LOC       0.96      0.95      0.96      4076\n",
      "       B-ORG       0.91      0.93      0.92      1218\n",
      "       I-ORG       0.93      0.96      0.94      5054\n",
      "       B-PER       0.98      0.99      0.98      1304\n",
      "       I-PER       0.98      0.98      0.98      2425\n",
      "\n",
      "   micro avg       0.95      0.96      0.96     16751\n",
      "   macro avg       0.95      0.96      0.96     16751\n",
      "weighted avg       0.95      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6199\n",
      "r_mega: 0.6199\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6198\n",
      "r_mega: 0.6198\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6197\n",
      "r_mega: 0.6197\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6195\n",
      "r_mega: 0.6195\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.95      0.97      2674\n",
      "       I-LOC       0.96      0.94      0.95      4076\n",
      "       B-ORG       0.90      0.94      0.92      1218\n",
      "       I-ORG       0.90      0.97      0.94      5054\n",
      "       B-PER       0.98      0.98      0.98      1304\n",
      "       I-PER       0.98      0.98      0.98      2425\n",
      "\n",
      "   micro avg       0.95      0.96      0.95     16751\n",
      "   macro avg       0.95      0.96      0.96     16751\n",
      "weighted avg       0.95      0.96      0.95     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6195\n",
      "r_mega: 0.6195\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6193\n",
      "r_mega: 0.6193\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6189\n",
      "r_mega: 0.6188\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6187\n",
      "r_mega: 0.6185\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.95      0.96      2674\n",
      "       I-LOC       0.97      0.93      0.95      4076\n",
      "       B-ORG       0.91      0.94      0.92      1218\n",
      "       I-ORG       0.93      0.93      0.93      5054\n",
      "       B-PER       0.97      0.99      0.98      1304\n",
      "       I-PER       0.96      0.99      0.98      2425\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16751\n",
      "   macro avg       0.95      0.95      0.95     16751\n",
      "weighted avg       0.95      0.95      0.95     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6186\n",
      "r_mega: 0.6183\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6192\n",
      "r_mega: 0.6177\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6211\n",
      "r_mega: 0.6156\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6226\n",
      "r_mega: 0.6138\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.96      0.97      2674\n",
      "       I-LOC       0.97      0.94      0.96      4076\n",
      "       B-ORG       0.92      0.94      0.93      1218\n",
      "       I-ORG       0.93      0.96      0.95      5054\n",
      "       B-PER       0.99      0.98      0.98      1304\n",
      "       I-PER       0.98      0.99      0.99      2425\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     16751\n",
      "   macro avg       0.96      0.96      0.96     16751\n",
      "weighted avg       0.96      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6232\n",
      "r_mega: 0.6130\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6244\n",
      "r_mega: 0.6116\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6259\n",
      "r_mega: 0.6100\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6273\n",
      "r_mega: 0.6085\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.95      0.97      2674\n",
      "       I-LOC       0.97      0.94      0.95      4076\n",
      "       B-ORG       0.92      0.94      0.93      1218\n",
      "       I-ORG       0.93      0.95      0.94      5054\n",
      "       B-PER       0.98      0.98      0.98      1304\n",
      "       I-PER       0.97      0.98      0.98      2425\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     16751\n",
      "   macro avg       0.96      0.96      0.96     16751\n",
      "weighted avg       0.96      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6275\n",
      "r_mega: 0.6083\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6285\n",
      "r_mega: 0.6073\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6298\n",
      "r_mega: 0.6061\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6307\n",
      "r_mega: 0.6052\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.96      0.97      2674\n",
      "       I-LOC       0.97      0.94      0.96      4076\n",
      "       B-ORG       0.91      0.94      0.93      1218\n",
      "       I-ORG       0.93      0.96      0.95      5054\n",
      "       B-PER       0.98      0.98      0.98      1304\n",
      "       I-PER       0.98      0.99      0.98      2425\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     16751\n",
      "   macro avg       0.96      0.96      0.96     16751\n",
      "weighted avg       0.96      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6309\n",
      "r_mega: 0.6051\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6316\n",
      "r_mega: 0.6044\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6323\n",
      "r_mega: 0.6038\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6329\n",
      "r_mega: 0.6033\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.96      0.97      2674\n",
      "       I-LOC       0.97      0.94      0.96      4076\n",
      "       B-ORG       0.93      0.93      0.93      1218\n",
      "       I-ORG       0.94      0.96      0.95      5054\n",
      "       B-PER       0.98      0.99      0.98      1304\n",
      "       I-PER       0.98      0.99      0.98      2425\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     16751\n",
      "   macro avg       0.96      0.96      0.96     16751\n",
      "weighted avg       0.96      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6329\n",
      "r_mega: 0.6033\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6333\n",
      "r_mega: 0.6030\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6335\n",
      "r_mega: 0.6028\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.98      0.96      0.97      2674\n",
      "       I-LOC       0.97      0.94      0.96      4076\n",
      "       B-ORG       0.94      0.94      0.94      1218\n",
      "       I-ORG       0.94      0.96      0.95      5054\n",
      "       B-PER       0.98      0.99      0.98      1304\n",
      "       I-PER       0.98      0.99      0.98      2425\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     16751\n",
      "   macro avg       0.96      0.96      0.96     16751\n",
      "weighted avg       0.96      0.96      0.96     16751\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6336\n",
      "r_mega: 0.6028\n",
      "\n",
      "Current weight of:\n",
      "r_lstm: 0.6336\n",
      "r_mega: 0.6028\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14490, training_loss=15.689221124010633, metrics={'train_runtime': 4054.121, 'train_samples_per_second': 114.363, 'train_steps_per_second': 3.574, 'total_flos': 5.661662440280064e+16, 'train_loss': 15.689221124010633, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=lambda x: compute_metrics(x,dataset),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    callbacks=[WeightLoggerCallback()]\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27531f-c1c9-4171-a302-142694de6836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
